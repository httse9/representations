import numpy as np
import jax.numpy as jnp
import random
import pickle
import gym
import gin
from minigrid_basics.reward_envs import maxent_mon_minigrid
from minigrid_basics.custom_wrappers import maxent_mdp_wrapper
from minigrid_basics.examples.visualizer import Visualizer
from minigrid_basics.examples.reward_shaper import RewardShaper
import matplotlib.pyplot as plt
import os
from os.path import join
import subprocess
import glob
from minigrid_basics.function_approximation.eigenlearner_FA import *

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

def set_random_seed(seed):
    np.random.seed(seed)
    random.seed(seed)

def load_dataset(args):
    with open(f"minigrid_basics/function_approximation/static_dataset/{args.env}_{args.obs_type}_2.pkl", "rb") as f:
        dataset = pickle.load(f)

    with open(f"minigrid_basics/function_approximation/static_dataset/{args.env}_{args.obs_type}_test.pkl", "rb") as f:
        test_set = jnp.array(pickle.load(f))

    return dataset, test_set

def get_learner(matrix):
    if matrix == "WGL":
        return WGLLearner
    elif matrix == "WT":
        return WTLearner
    elif matrix == 'DR':
        return DRLearner
    else:
        raise NotImplementedError()

def eigenlearning_tabular(args, env):
    
    ### load dataset
    dataset, test_set = load_dataset(args)

    ### for visualizing eigvec
    visualizer = Visualizer(env)
    cmap = "rainbow"   

    ### init learner
    learner = get_learner(args.matrix)(env, dataset, test_set, args)
    learner.init_learn()

    ### visualize true eigvec
    # print(learner.true_eigvec)
    # visualizer.visualize_shaping_reward_2d(learner.true_eigvec, ax=None, normalize=True, vmin=0, vmax=1, cmap=cmap)
    # plt.show()
    # quit()

    ### learn
    learner.learn()

    ### record
    """
    TODO:
    1. save learned net (TODO, need after hyperparameter tuning)
    2. cosine similarity (both raw data and plot) (done)
    3. eigvec norm (both raw data and plot) (done)
    4. true eigvec, learned eigvec (both raw data and plot) (done)
    
    Experiment:
    Objective: generalize DR eigvec learning to function approx.
    Now we have a method to learn.
    Prove that they indeed learn the ground-truth eigvec.

    A. Using ground-truth dataset with all transitions (only for our sanity check, no need show in paper)
    B. Use dataset generated by running uniform random policy. (need hyperparameter search)
    C. Use learned eigenvector for reward shaping
    D. Learn eigvec while learning value/policy (next paper)

    1. Report cos sim between learned and true eigvec, for diff obs type, for diff random seeds (10 seeds, 95% conf. int.)
      - All obs type in the same plot
      - Two rows: top row no low-reward region, bottom row has
      - Columns: 4 env settings
      - Each plot, cosine similarity for all obs type, with 95% conf. int.
    2. Visualization of learned and true eigvec (for all 8 envs) (Appendix)
      - Average learned eigvec over 10 seeds. Plot side by side
      - Two rows: learned, true
      - Columns: 8 envs
      - repeat this by three for one-hot, coordinates, image
    """

    ### save plots
    plt.axhline(1.0, linestyle="--", color='k')
    plt.plot(learner.cos_sims)
    plt.xlabel("Time Steps (x100)")
    plt.ylabel("Cosine Similarity")
    plt.tight_layout()
    plt.savefig(join(plot_path, f"{run_name}_cossim.png"))
    plt.clf()

    plt.axhline(env.num_states, linestyle="--", color="k")
    plt.plot(learner.norms)
    plt.xlabel("Time Steps (x100)")
    plt.ylabel("Eigvec Norm")
    plt.tight_layout()
    plt.savefig(join(plot_path, f"{run_name}_eigvec-norm.png"))
    plt.clf()

    eigvec_pred = learner.eigvec()  # predicted eigenvector
    plt.subplot(1, 2, 1)
    visualizer.visualize_shaping_reward_2d(learner.true_eigvec, ax=None, normalize=True, vmin=0, vmax=1, cmap=cmap)
    plt.title("True Eigvec")
    plt.subplot(1, 2, 2)
    visualizer.visualize_shaping_reward_2d(eigvec_pred, ax=None, normalize=True, vmin=0, vmax=1, cmap=cmap)
    plt.title("Learned Eigvec")
    plt.tight_layout()
    plt.savefig(join(plot_path, f"{run_name}_eigvec.png"))
    plt.clf()

    ### save raw data
    data = dict(
        cos_sims=learner.cos_sims,
        norms=learner.norms,
        true_eigvec=learner.true_eigvec,
        eigvec=eigvec_pred
    )
    with open(join(data_path, f"{run_name}.pkl"), "wb") as f:
        pickle.dump(data, f)

    ### save learned neural net
    if args.save_model:
        raise NotImplementedError()


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--env", default="fourrooms_2", type=str, help="Specify environment.")
    parser.add_argument("--obs_type", default="onehot", type=str)
    parser.add_argument("--matrix", type=str, default="WGL", help="Which matrix's eigenvector to learn [WGL/WT/DR]")
    
    parser.add_argument("--lambd", help="lambda for DR", default=1.0, type=float)
    parser.add_argument("--step_size_start", default=1e-3, type=float, help="Starting step size")
    parser.add_argument("--step_size_end", default=None, type=float, help="Ending step size")
    parser.add_argument("--grad_norm_clip", default=0.5, type=float, help="Ending step size")

    parser.add_argument("--n_epochs", type=int, default=10000)
    parser.add_argument("--log_interval", default=100, type=int, help="interval to compute cosine similarity")

    parser.add_argument("--save_model", action="store_true", help="Whether to save trained network.")

    parser.add_argument("--seed", type=int, default=0)
    args = parser.parse_args()

    if args.step_size_end is None:
        args.step_size_end = args.step_size_start

    # create env
    set_random_seed(args.seed)
    gin.parse_config_file(os.path.join(maxent_mon_minigrid.GIN_FILES_PREFIX, f"{args.env}.gin"))
    env_id = maxent_mon_minigrid.register_environment()
    env = gym.make(env_id, disable_env_checker=True)
    env = maxent_mdp_wrapper.MDPWrapper(env, goal_absorbing=True)

    # create dir for saving results
    path = join("minigrid_basics", "function_approximation", "experiments", args.env, args.obs_type, args.matrix)
    plot_path = join(path, "plots")
    data_path = join(path, "data")
    os.makedirs(plot_path, exist_ok=True)
    os.makedirs(data_path, exist_ok=True)
    run_name = f"{args.lambd}-{args.step_size_start}-{args.step_size_end}-{args.grad_norm_clip}-{args.seed}"

    # learn
    eigenlearning_tabular(args, env)