import numpy as np
import jax.numpy as jnp
import random
import pickle
import gym
import gin
from minigrid_basics.reward_envs import maxent_mon_minigrid
from minigrid_basics.custom_wrappers import maxent_mdp_wrapper
from minigrid_basics.examples.visualizer import Visualizer
from minigrid_basics.examples.reward_shaper import RewardShaper
import matplotlib.pyplot as plt
import os
from os.path import join
import subprocess
import glob
from minigrid_basics.function_approximation.WGL_ALLO import WGLALLOLearner

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

def set_random_seed(seed):
    np.random.seed(seed)
    random.seed(seed)

def load_dataset(args):
    with open(f"minigrid_basics/function_approximation/dataset/{args.env}_dataset.pkl", "rb") as f:
        dataset = pickle.load(f)[args.obs_type]

    with open(f"minigrid_basics/function_approximation/dataset/{args.env}_testset.pkl", "rb") as f:
        test_set = jnp.array(pickle.load(f))[args.obs_type]

    return dataset, test_set



def eigenlearning(args, env):
    
    ### load dataset
    dataset, test_set = load_dataset(args)

    ### for visualizing eigvec
    visualizer = Visualizer(env)
    cmap = "rainbow"   

    ### init learner
    learner = WGLALLOLearner(env, dataset, test_set, args)
    learner.init_learn()

    ### learn
    learner.learn()

    """
    TODO:
    1. save learned net (TODO, need after hyperparameter tuning)
    2. cosine similarity (both raw data and plot) (done)
    3. eigvec norm (both raw data and plot) (done)
    4. true eigvec, learned eigvec (both raw data and plot) (done)
    
    Experiment:
    Objective: generalize DR eigvec learning to function approx.
    Now we have a method to learn.
    Prove that they indeed learn the ground-truth eigvec.

    Progression:
    A. Using ground-truth dataset with all transitions (only for our sanity check, no need show in paper)
    B. Use dataset generated by running uniform random policy. (need hyperparameter search)
    C. Use learned eigenvector for reward shaping
    """

    ### save plots
    plt.axhline(1.0, linestyle="--", color='k')
    if args.eig_dim > 1:
        cos_sims = np.array(learner.cos_sims)
        for i in range(args.eig_dim):
            plt.plot(cos_sims[:, i], label=f"dim {i + 1}")
        plt.legend()
    else:
        plt.plot(learner.cos_sims)
    plt.xlabel("Time Steps (x100)")
    plt.ylabel("Cosine Similarity")
    plt.tight_layout()
    plt.savefig(join(plot_path, f"{run_name}_cossim.png"))
    plt.clf()

    plt.axhline(env.num_states, linestyle="--", color="k")
    plt.plot(learner.norms)
    plt.xlabel("Time Steps (x100)")
    plt.ylabel("Eigvec Norm")
    plt.tight_layout()
    plt.savefig(join(plot_path, f"{run_name}_eigvec-norm.png"))
    plt.clf()

    eigvec_pred = learner.eigvec()  # predicted eigenvector
    plt.figure(figsize=(2 * args.eig_dim, 4))
    for i in range(args.eig_dim):
        plt.subplot(2, args.eig_dim, i + 1)
        visualizer.visualize_shaping_reward_2d(learner.true_eigvec[:, i], ax=None, normalize=True, vmin=0, vmax=1, cmap=cmap)

        plt.subplot(2, args.eig_dim, i + 1 + args.eig_dim)
        visualizer.visualize_shaping_reward_2d(eigvec_pred[:, i], ax=None, normalize=True, vmin=0, vmax=1, cmap=cmap)

    plt.tight_layout()
    plt.savefig(join(plot_path, f"{run_name}_eigvec.png"))
    plt.clf()


    ### save raw data
    data = dict(
        cos_sims=learner.cos_sims,
        norms=learner.norms,
        true_eigvec=learner.true_eigvec,
        eigvec=eigvec_pred
    )
    with open(join(data_path, f"{run_name}.pkl"), "wb") as f:
        pickle.dump(data, f)

    ### save learned neural net
    if args.save_model:
        raise NotImplementedError()


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--env", default="fourrooms_2", type=str, help="Specify environment.")
    parser.add_argument("--obs_type", default="onehot", type=str)
    
    parser.add_argument("--lambd", help="lambda for DR", default=1.0, type=float)
    parser.add_argument("--step_size_start", default=1e-4, type=float, help="Starting step size")
    parser.add_argument("--step_size_end", default=None, type=float, help="Ending step size")
    parser.add_argument("--grad_norm_clip", default=0.5, type=float, help="Ending step size")

    parser.add_argument("--n_epochs", type=int, default=400, help="Number of passes thru dataset")
    parser.add_argument("--batch_size", default=250, help="Batch size", type=int)
    parser.add_argument("--log_interval", default=100, type=int, help="interval to compute cosine similarity")

    parser.add_argument("--eig_dim", type=int, default=1, help="How many dimension of laplacian representation to learn")

    parser.add_argument("--save_model", action="store_true", help="Whether to save trained network.")

    parser.add_argument("--seed", type=int, default=0)
    args = parser.parse_args()

    if args.step_size_end is None:
        args.step_size_end = args.step_size_start

    # create env
    set_random_seed(args.seed)
    gin.parse_config_file(os.path.join(maxent_mon_minigrid.GIN_FILES_PREFIX, f"{args.env}.gin"))
    env_id = maxent_mon_minigrid.register_environment()
    env = gym.make(env_id, disable_env_checker=True)
    env = maxent_mdp_wrapper.MDPWrapper(env, goal_absorbing=True)

    # create dir for saving results
    path = join("minigrid_basics", "function_approximation", "experiments_test", args.env, args.obs_type)
    plot_path = join(path, "plots")
    data_path = join(path, "data")
    os.makedirs(plot_path, exist_ok=True)
    os.makedirs(data_path, exist_ok=True)
    run_name = f"{args.lambd}-{args.step_size_start}-{args.step_size_end}-{args.grad_norm_clip}-{args.seed}"

    # learn
    eigenlearning(args, env)